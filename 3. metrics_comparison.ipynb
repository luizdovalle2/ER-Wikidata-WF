{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1227f63",
   "metadata": {},
   "source": [
    "# Evaluation of coverage outputs (best F2 / recall / threshold)\n",
    "\n",
    "## Purpose\n",
    "This notebook loads multiple saved *candidate-pair result files* (one per configuration/metric),\n",
    "evaluates them at several `match_count` thresholds, and produces a summary table with the\n",
    "**best F2**, **recall**, and the corresponding **threshold**, as reported in the article.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c989bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Ground-truth pairs (must contain: plLabel, alias, and index)\n",
    "GOLD_FILE = \"wd_dataset.parquet\"   # adjust if your gold is elsewhere\n",
    "\n",
    "# Thresholds to test (matching your previous loop: 0.5..1.0) (minimum coverage to be considered a match)\n",
    "THRESHOLDS = [t / 10 for t in range(6, 11)]\n",
    "\n",
    "# Number of samples from each class (balanced dataset size = 2*N) to separate the training data (used in other metrics)\n",
    "N_SAMPLES_PER_CLASS = 50\n",
    "\n",
    "# \n",
    "RANDOM_SEED = 42\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5482eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = pd.read_parquet(GOLD_FILE)\n",
    "\n",
    "required_gold_cols = {\"polish_label\", \"name_variant\", \"person\"}\n",
    "missing = required_gold_cols - set(df_gold.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Gold file missing columns: {missing}\")\n",
    "\n",
    "# Gold set of true pairs for fast set operations\n",
    "gold_pairs = set(map(tuple, df_gold[[\"polish_label\", \"name_variant\"]].to_records(index=False)))\n",
    "\n",
    "print(\"Gold pairs:\", len(gold_pairs))\n",
    "df_gold.head(3)\n",
    "\n",
    "# value possible previously used for threshold for lower memory usage\n",
    "prev_mem_based_threshold = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9aedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# Load all coverage output files and merge into single feature matrix\n",
    "# ============================================================================\n",
    "\n",
    "COVERAGE_DIR = Path(\"coverage_outputs\")\n",
    "\n",
    "# Find all parquet files in coverage_outputs/\n",
    "coverage_files = sorted(glob.glob(str(COVERAGE_DIR / \"*.parquet\")))\n",
    "\n",
    "print(f\"Found {len(coverage_files)} coverage files:\")\n",
    "for f in coverage_files:\n",
    "    print(f\"  - {Path(f).name}\")\n",
    "\n",
    "# Load each file and extract the coverage column (named by config)\n",
    "dfs = []\n",
    "\n",
    "for fpath in coverage_files:\n",
    "    df_temp = pd.read_parquet(fpath)\n",
    "    print(len(df_temp))\n",
    "    # Extract config name from filename: coverage_<CONFIG>.parquet -> CONFIG\n",
    "    config_name = Path(fpath).stem.replace(\"coverage_\", \"\")\n",
    "    \n",
    "    # Keep only the index columns + the coverage score, rename coverage column\n",
    "    coverage_col = \"coverage_value\"\n",
    "  \n",
    "    \n",
    "    df_temp = df_temp[[\"polish_label\", \"name_variant\",  \"coverage_value\"]].copy()\n",
    "    df_temp = df_temp.rename(columns={coverage_col: f\"coverage_{config_name}\"})\n",
    "    dfs.append(df_temp)\n",
    "\n",
    "# ============================================================================\n",
    "# Merge all coverage DataFrames on index + label + variant\n",
    "# (outer join to keep all pairs, even if missing in some configs)\n",
    "# ============================================================================\n",
    "\n",
    "if dfs:\n",
    "\n",
    "    df_merged = dfs[0]\n",
    "    \n",
    "    for df_next in dfs[1:]:\n",
    "        df_merged = pd.merge(\n",
    "            df_merged,\n",
    "            df_next,\n",
    "            on=[\"polish_label\", \"name_variant\"],\n",
    "            how=\"outer\"\n",
    "        )\n",
    "        df_merged= df_merged.drop_duplicates()\n",
    "    \n",
    "    print(f\"\\nMerged shape: {df_merged.shape}\")\n",
    "    print(f\"Columns: {df_merged.columns.tolist()}\")\n",
    "    \n",
    "   \n",
    "    # =========================================================================\n",
    "    # Load gold pairs and add truth label\n",
    "    # =========================================================================\n",
    "    df_gold = pd.read_parquet(\"wd_dataset.parquet\")\n",
    "    \n",
    "    # Create a gold set for matching\n",
    "    gold_pairs = set(map(tuple, df_gold[[\"polish_label\", \"name_variant\"]].to_records(index=False)))\n",
    "    \n",
    "    # Add truth column: True if (label, variant) is in gold set\n",
    "    df_merged[\"truth\"] = df_merged.apply(\n",
    "        lambda row: (row[\"polish_label\"], row[\"name_variant\"]) in gold_pairs,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nAdded truth label:\")\n",
    "    print(f\"  Positive (True) pairs: {df_merged['truth'].sum()}\")\n",
    "    print(f\"  Negative (False) pairs: {(~df_merged['truth']).sum()}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # Optional: Add text length features\n",
    "    # =========================================================================\n",
    "    df_merged[\"len_label\"] = df_merged[\"polish_label\"].str.split().str.len()\n",
    "    df_merged[\"len_variant\"] = df_merged[\"name_variant\"].str.split().str.len()\n",
    "    \n",
    "    print(f\"\\nFinal df_merged shape: {df_merged.shape}\")\n",
    "    print(f\"Final columns: {df_merged.columns.tolist()}\")\n",
    "    \n",
    "    df_merged.head(10)\n",
    "\n",
    "\n",
    "     # =========================================================================\n",
    "    # Fill missing coverage values with previously defined threshold (for lower memory usage)\n",
    "    # =========================================================================\n",
    "    if prev_mem_based_threshold > 0:\n",
    "        coverage_cols = [c for c in df_merged.columns if c.startswith(\"coverage_\")]\n",
    "        for col in coverage_cols:\n",
    "            df_merged[col] = df_merged[col].fillna(prev_mem_based_threshold)\n",
    "    \n",
    "else:\n",
    "    print(\"ERROR: No coverage files found or loaded successfully\")\n",
    "\n",
    "\n",
    "# Add merge_col for later filtering\n",
    "df_merged[\"merge_col\"] = df_merged.apply(lambda row: tuple([row.polish_label, row.name_variant]), axis=1)\n",
    "\n",
    "df_gold[\"merge_col\"] = df_gold.apply(lambda row: tuple([row.polish_label, row.name_variant]), axis=1)\n",
    "\n",
    "# save merged df for future \n",
    "df_merged.to_parquet(\"df_merged.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f656307",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_full = df_merged[\"truth\"].astype(int).values\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "pos_idx = np.where(y_full == 1)[0]\n",
    "neg_idx = np.where(y_full == 0)[0]\n",
    "\n",
    "n = min(N_SAMPLES_PER_CLASS, len(pos_idx), len(neg_idx))\n",
    "\n",
    "sample_idx = np.concatenate([\n",
    "    rng.choice(pos_idx, size=n, replace=False),\n",
    "    rng.choice(neg_idx, size=n, replace=False),\n",
    "])\n",
    "\n",
    "\n",
    "mask = ~np.isin(np.arange(len(df_merged)), sample_idx)\n",
    "df_not_in_sample = df_merged[mask]\n",
    "\n",
    "# Or with pandas:\n",
    "df_merged_test = df_merged.loc[~df_merged.index.isin(sample_idx)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4617389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fbeta(precision: float, recall: float, beta: float = 2.0) -> float:\n",
    "    \"\"\"\n",
    "    Compute F_beta score (weighted harmonic mean of precision and recall).\n",
    "    Default is F2 (recall-weighted).\n",
    "    \n",
    "    Formula:\n",
    "    F_β = (1 + β²) × (P × R) / (β² × P + R)\n",
    "    \"\"\"\n",
    "    if precision <= 0.0 and recall <= 0.0:\n",
    "        return 0.0\n",
    "    \n",
    "    beta2 = beta * beta\n",
    "    numerator = (1 + beta2) * precision * recall\n",
    "    denominator = (beta2 * precision) + recall\n",
    "    \n",
    "    return numerator / denominator if denominator > 0 else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_result_df(df_result: pd.DataFrame, gold_pairs: set, df_gold: pd.DataFrame, thresholds, coverage_col_name) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Evaluate a single result dataframe over multiple thresholds.\n",
    "\n",
    "    Returns a dataframe with one row per threshold containing:\n",
    "    precision, recall, f2, counts, retrieved amount, etc.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for thr in thresholds:\n",
    "        # Filter by coverage threshold\n",
    "        df_filt = df_result[df_result[coverage_col_name] >= thr]\n",
    "\n",
    "        # False negatives: gold indices not retrieved (same logic you used)\n",
    "        false_negative = df_gold[~df_gold[\"merge_col\"].isin(df_filt[\"merge_col\"])]\n",
    "        # Predicted pair set at this threshold\n",
    "        pred_pairs = set(map(tuple, df_filt[[\"polish_label\", \"name_variant\"]].to_records(index=False)))\n",
    "\n",
    "        # Set-based TP/FP\n",
    "        tp_set = pred_pairs & gold_pairs\n",
    "        fp_set = pred_pairs - gold_pairs\n",
    "\n",
    "        tp = len(tp_set)\n",
    "        fp = len(fp_set)\n",
    "        fn = len(false_negative)\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f2 = fbeta(precision, recall, beta=2.0)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": thr,\n",
    "                \"retrieved\": len(df_filt),\n",
    "                \"tp\": tp,\n",
    "                \"fp\": fp,\n",
    "                \"fn\": fn,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall,\n",
    "                \"f2\": f2,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_cols = [c for c in df_merged.columns if c.startswith(\"coverage_\")]\n",
    "\n",
    "summary_rows = []\n",
    "for c in coverage_cols:\n",
    "        \n",
    "    df_res = df_merged[[\"name_variant\", \"merge_col\", \"polish_label\",c]]\n",
    "\n",
    "    eval_table = evaluate_result_df(df_res, gold_pairs=gold_pairs, df_gold=df_gold, thresholds=THRESHOLDS, coverage_col_name=c)\n",
    "\n",
    "    # Select best threshold by F2 (ties broken by higher recall, then higher precision)\n",
    "    best = (\n",
    "        eval_table.sort_values([\"f2\", \"recall\", \"precision\"], ascending=False)\n",
    "        .iloc[0]\n",
    "        .to_dict()\n",
    "    )\n",
    "    print(eval_table)\n",
    "    summary_rows.append(\n",
    "        {\n",
    "            \"file\": os.path.basename(c.replace(\"coverage_\", \"\")),\n",
    "            \"best_threshold\": best[\"threshold\"],\n",
    "            \"precision\": best[\"precision\"],\n",
    "            \"recall\": best[\"recall\"],\n",
    "            \"f2\": best[\"f2\"],\n",
    "            \"retrieved\": int(best[\"retrieved\"]),\n",
    "            \"tp\": int(best[\"tp\"]),\n",
    "            \"fp\": int(best[\"fp\"]),\n",
    "            \"fn\": int(best[\"fn\"]),\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Export summary of best values\n",
    "df_summary = pd.DataFrame(summary_rows).sort_values([\"f2\", \"recall\"], ascending=False)\n",
    "df_summary.to_csv(\"summary_best_values.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
