{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79cd87d7",
   "metadata": {},
   "source": [
    "# Random Forest & XGBoost â€” training + cross-validation\n",
    "\n",
    "This notebook trains supervised classifiers to predict whether a `(polish_label, name_variant)` pair is a true match, using precomputed string/phonetic metric features.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa02d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import fbeta_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Categorical\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d66844f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf893f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Number of samples from each class (balanced dataset size = 2*N)\n",
    "N_SAMPLES_PER_CLASS = 500\n",
    "\n",
    "# Number of folds in stratified K-fold CV\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Decision thresholds applied to predicted probabilities\n",
    "THRESHOLDS = [0.35, 0.40, 0.45, 0.50]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c22ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = pd.read_parquet(\"merged_weigths.parquet\")\n",
    "df_gold = pd.read_parquet(\"wd_dataset.parquet\")\n",
    "df_gold[\"merge_col\"] = df_gold.apply(lambda row: tuple([row.polish_label, row.name_variant]), axis=1)\n",
    "\n",
    "print(\"df_gold:\", df_gold.shape)\n",
    "df_merged= pd.read_parquet(\"df_merged.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f984ec",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Define X (features) and y (target label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110297d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_full = df_merged[\"truth\"].astype(int).values\n",
    "\n",
    "# Feature columns: renamed metric columns (coverages, and lengths of strings list)\n",
    "feature_cols = [c for c in df_merged.columns if c.startswith((\"coverage\", \"len\"))]\n",
    "X_full = df_merged[feature_cols].values\n",
    "\n",
    "print(\"Number of features:\", len(feature_cols))\n",
    "print(\"Feature names:\", feature_cols)\n",
    "print(\"Class distribution [0, 1]:\", np.bincount(y_full))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a25923",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Create balanced training subset\n",
    "\n",
    "Sample equal numbers of positives and negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "pos_idx = np.where(y_full == 1)[0]\n",
    "neg_idx = np.where(y_full == 0)[0]\n",
    "\n",
    "n = min(N_SAMPLES_PER_CLASS, len(pos_idx), len(neg_idx))\n",
    "\n",
    "sample_idx = np.concatenate([\n",
    "    rng.choice(pos_idx, size=n, replace=False),\n",
    "    rng.choice(neg_idx, size=n, replace=False),\n",
    "])\n",
    "rng.shuffle(sample_idx)\n",
    "\n",
    "X_sampled = X_full[sample_idx]\n",
    "y_sampled = y_full[sample_idx]\n",
    "\n",
    "print(\"Sampled shape:\", X_sampled.shape)\n",
    "print(\"Sampled class counts [0, 1]:\", np.bincount(y_sampled))\n",
    "print(\"Positive ratio:\", np.mean(y_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c89569",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Random Forest: Bayesian search space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5170bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_search_space = {\n",
    "    \"n_estimators\": Integer(100, 1000),\n",
    "    \"max_depth\": Integer(5, 30),\n",
    "    \"min_samples_split\": Integer(2, 20),\n",
    "    \"min_samples_leaf\": Integer(1, 10),\n",
    "    \"max_features\": Categorical([\"sqrt\", \"log2\"]),\n",
    "}\n",
    "\n",
    "print(\"RF search space defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615c1441",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Random Forest: Stratified K-Fold + threshold sweep (main loop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_spaces_rf = {\n",
    "    'n_estimators': Integer(100, 1000),\n",
    "    'max_depth': Integer(5, 30),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'min_samples_leaf': Integer(1, 10),\n",
    "    'max_features': Categorical(['sqrt', 'log2'])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting {N_FOLDS}-fold cross-validation...\")\n",
    "print(f\"Testing thresholds: {THRESHOLDS}\")\n",
    "print(f\"Sample size: {len(y_sampled)} | Train per fold: {int(len(y_sampled) * (N_FOLDS - 1) / N_FOLDS)}\")\n",
    "\n",
    "# Model definition (here you can easily swap RandomForest for XGBoost, etc.)\n",
    "\n",
    "# Stratified K-Fold ensures the class proportion is preserved in each fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "beta = 2\n",
    "\n",
    "# Containers for results\n",
    "results_by_threshold = {t: {\n",
    "    \"val_f2\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f2\": []\n",
    "} for t in THRESHOLDS}\n",
    "\n",
    "best_params_history = []\n",
    "\n",
    "print(f\"RandomForestClassifier: Starting {N_FOLDS}-fold cross-validation...\")\n",
    "print(f\"Thresholds being tested: {THRESHOLDS}\")\n",
    "print(f\"Sample size: {len(y_sampled)} (Training per fold: ~{int(len(y_sampled) * 0.8)})\")\n",
    "\n",
    "fold_no = 1\n",
    "for train_idx_local, test_idx_local in skf.split(X_sampled, y_sampled):\n",
    "    # 1) Prepare the datasets\n",
    "    X_train_fold = X_sampled[train_idx_local]\n",
    "    y_train_fold = y_sampled[train_idx_local]\n",
    "    X_test_fold = X_sampled[test_idx_local]\n",
    "    y_test_fold = y_sampled[test_idx_local]\n",
    "\n",
    "    # Masks and the global set\n",
    "    global_train_idx = sample_idx[train_idx_local]\n",
    "    mask_train = np.zeros(len(df_merged), dtype=bool)\n",
    "    mask_train[global_train_idx] = True\n",
    "    X_test_global = X_full[~mask_train]\n",
    "\n",
    "    # 2) Bayesian hyperparameter optimization\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=RandomForestClassifier(n_jobs=-1, random_state=RANDOM_SEED),\n",
    "        search_spaces=search_spaces_rf,\n",
    "        scoring='f1',  # We optimize F1 (harmonic mean), because F2 isn't available as a standard string scorer\n",
    "        cv=3,\n",
    "        n_iter=15,  # Reduced a bit for speed; you can set 20\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "    opt.fit(X_train_fold, y_train_fold)\n",
    "    best_model_rf = opt.best_estimator_\n",
    "    best_params_rf = dict(opt.best_params_)\n",
    "    best_params_history.append(best_params_rf)\n",
    "    print(f\"\\nFold {fold_no}: Params: {best_params_rf}\")\n",
    "\n",
    "    # Compute probabilities ONCE for this fold/model\n",
    "    probs_local = best_model_rf.predict_proba(X_test_fold)[:, 1]\n",
    "    probs_global = best_model_rf.predict_proba(X_test_global)[:, 1]\n",
    "\n",
    "    # Prepare Ground Truth for filtering (shared within the fold)\n",
    "    train_positive_indices = global_train_idx[y_train_fold == 1]\n",
    "    train_positive_merge_cols = df_merged.iloc[train_positive_indices][\"merge_col\"]\n",
    "    df_res_filtered = df_gold[~df_gold.merge_col.isin(train_positive_merge_cols)]\n",
    "\n",
    "    # 3) Loop over thresholds (evaluate many variants at once)\n",
    "    best_fold_stats = {\"f2_global\": -1, \"thr\": 0, \"p\": 0, \"r\": 0, \"val_f2\": 0}\n",
    "    for t in THRESHOLDS:\n",
    "        # --- Local evaluation (Validation) ---\n",
    "        preds_local_t = (probs_local >= t).astype(int)\n",
    "        val_f2 = fbeta_score(y_test_fold, preds_local_t, beta=2, zero_division=0)\n",
    "        val_acc = accuracy_score(y_test_fold, preds_local_t)\n",
    "\n",
    "        # --- Global evaluation (\"Rest of the world\") ---\n",
    "        val_preds_global = (probs_global >= t).astype(int)\n",
    "\n",
    "        # Business logic (Set-based metrics)\n",
    "        df_test_fold = df_merged[~mask_train].copy()\n",
    "        df_test_fold[\"result\"] = val_preds_global\n",
    "\n",
    "        res_filt = df_test_fold[df_test_fold[\"result\"] == 1]\n",
    "        org_set = set(df_res_filtered[[\"polish_label\", \"name_variant\"]].itertuples(index=False, name=None))\n",
    "        new_set = set(res_filt[[\"polish_label\", \"name_variant\"]].itertuples(index=False, name=None))\n",
    "\n",
    "        true_positive_count = len(new_set & org_set)\n",
    "        false_positive_count = len(new_set - org_set)\n",
    "        false_negative_df = df_res_filtered.loc[~df_res_filtered[\"merge_col\"].isin(res_filt[\"merge_col\"])]\n",
    "        false_negative_count = len(false_negative_df)\n",
    "\n",
    "        prec = true_positive_count / (true_positive_count + false_positive_count) if (\n",
    "            true_positive_count + false_positive_count) > 0 else 0\n",
    "        rec = true_positive_count / (true_positive_count + false_negative_count) if (\n",
    "            true_positive_count + false_negative_count) > 0 else 0\n",
    "        f2_global = (1 + beta ** 2) * (prec * rec) / (beta ** 2 * prec + rec) if (beta ** 2 * prec + rec) > 0 else 0\n",
    "\n",
    "        # Store results for this threshold\n",
    "        results_by_threshold[t][\"val_f2\"].append(val_f2)\n",
    "        results_by_threshold[t][\"accuracy\"].append(val_acc)\n",
    "        results_by_threshold[t][\"precision\"].append(prec)\n",
    "        results_by_threshold[t][\"recall\"].append(rec)\n",
    "        results_by_threshold[t][\"f2\"].append(f2_global)\n",
    "\n",
    "        # --- Update the best result within this fold ---\n",
    "        if f2_global > best_fold_stats[\"f2_global\"]:\n",
    "            best_fold_stats = {\n",
    "                \"f2_global\": f2_global,\n",
    "                \"thr\": t,\n",
    "                \"p\": prec,\n",
    "                \"r\": rec,\n",
    "                \"val_f2\": val_f2\n",
    "            }\n",
    "\n",
    "    # Print per fold\n",
    "    print(\n",
    "        f\"Fold {fold_no}: Best Thr: {best_fold_stats['thr']:.2f} | Val F2: {best_fold_stats['val_f2']:.4f} | Global F2: {best_fold_stats['f2_global']:.4f} (P: {best_fold_stats['p']:.2f}, R: {best_fold_stats['r']:.2f})\"\n",
    "    )\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26970faa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6942cd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMMARY OF PARAMETERS (Mode / Most Frequent) ---\n",
    "params_df = pd.DataFrame(best_params_history)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HYPERPARAMETER ANALYSIS (Best Params from 5 folds)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# We compute the most frequent value (mode) for each parameter\n",
    "most_common_params = params_df.mode().iloc[0].to_dict()\n",
    "print(\"Most frequent configuration (Mode):\")\n",
    "for k, v in most_common_params.items():\n",
    "    # If parameters are numbers, format nicely; if strings, leave as-is\n",
    "    val_str = f\"{int(v)}\" if isinstance(v, (int, np.integer)) else f\"{v}\"\n",
    "    print(f\"  {k}: {val_str}\")\n",
    "\n",
    "\n",
    "# --- SUMMARY OF RESULTS FOR EACH THRESHOLD ---\n",
    "# 1) First, we find the winning threshold (highest mean F2)\n",
    "best_avg_f2 = -1\n",
    "best_t_final = None\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    current_avg_f2 = np.mean(results_by_threshold[t]['f2'])\n",
    "    if current_avg_f2 > best_avg_f2:\n",
    "        best_avg_f2 = current_avg_f2\n",
    "        best_t_final = t\n",
    "\n",
    "\n",
    "# 2) Display results, marking the winner in the header\n",
    "for t in THRESHOLDS:\n",
    "    res = results_by_threshold[t]\n",
    "\n",
    "    # Check whether this is the winner\n",
    "    if t == best_t_final:\n",
    "        header_suffix = \" ðŸ† \"\n",
    "    else:\n",
    "        header_suffix = \"\"\n",
    "\n",
    "    print(f\"\\n=== {header_suffix} FINAL RESULTS (Mean +/- Std) for Threshold: {t:.2f} ===\")\n",
    "    print(f\"Val F2 (Local): {np.mean(res['val_f2']):.4f} Â± {np.std(res['val_f2']):.4f}\")\n",
    "    print(f\"Accuracy (Val): {np.mean(res['accuracy']):.4f} Â± {np.std(res['accuracy']):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Precision:      {np.mean(res['precision']):.4f} Â± {np.std(res['precision']):.4f}\")\n",
    "    print(f\"Recall:         {np.mean(res['recall']):.4f} Â± {np.std(res['recall']):.4f}\")\n",
    "    print(f\"F2-Score:       {np.mean(res['f2']):.4f} Â± {np.std(res['f2']):.4f}\")\n",
    "\n",
    "\n",
    "# precision: 0.6469689849624061\n",
    "# recall: 0.7762898223851141\n",
    "# F2: 0.7464487096074605\n",
    "# threshold: 0.4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc1f97",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05abebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the nested dictionary `results_by_threshold` into a flat DataFrame\n",
    "data_rows = []\n",
    "\n",
    "for t, metrics in results_by_threshold.items():\n",
    "    # `metrics` is a dictionary of lists, e.g. [0.7, 0.72, ...] for each fold\n",
    "    n_folds_actual = len(metrics['f2'])  # should be 5\n",
    "\n",
    "    for i in range(n_folds_actual):\n",
    "        # Add rows for each metric we care about\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Global F2', 'Score': metrics['f2'][i]})\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Precision', 'Score': metrics['precision'][i]})\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Recall', 'Score': metrics['recall'][i]})\n",
    "        # Optionally add Val F2 if you want to compare it\n",
    "        # data_rows.append({'Threshold': t, 'Fold': i+1, 'Metric': 'Local Val F2', 'Score': metrics['val_f2'][i]})\n",
    "\n",
    "df_metrics = pd.DataFrame(data_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474ec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Style configuration\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "plt.figure(figsize=(10, 7))  # Slightly taller chart to fit the legend at the bottom\n",
    "\n",
    "\n",
    "# 3. Define colors (Manual for full controlâ€”matching your screenshot)\n",
    "# Global F2 (Blue), Precision (Orange), Recall (Green)\n",
    "colors = sns.color_palette(\"deep\", 3)\n",
    "\n",
    "\n",
    "# 4. Draw the plot\n",
    "ax = sns.barplot(\n",
    "    data=df_metrics,\n",
    "    x=\"Threshold\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Metric\",\n",
    "    palette=colors,  # Use the \"deep\" palette (as in the screenshot) instead of viridis\n",
    "    capsize=.05,  # Slightly more subtle error bar caps\n",
    "    errorbar=\"sd\",\n",
    "    edgecolor=\"0.3\",  # Dark gray border for contrast\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Titles and axes\n",
    "plt.title('Impact of Threshold on Model Performance (5-Fold CV)', fontsize=16, weight='bold', y=1.02)\n",
    "plt.xlabel('Decision Threshold', fontsize=13, labelpad=10)\n",
    "plt.ylabel('Score (0-1)', fontsize=13, labelpad=10)\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "\n",
    "# 6. IMPROVED LEGEND (Placed below the plot)\n",
    "plt.legend(\n",
    "    title=None,  # Remove the unnecessary \"Metric\" title\n",
    "    loc='upper center',  # Legend anchor point\n",
    "    bbox_to_anchor=(0.5, -0.15),  # Shift below the X axis\n",
    "    ncol=3,  # All items on one line\n",
    "    frameon=False,  # No border/frame\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Values on bars (Annotation)\n",
    "for container in ax.containers:\n",
    "    # Get bar heights and format them\n",
    "    labels = [f'{v.get_height():.2f}' for v in container]\n",
    "\n",
    "    ax.bar_label(\n",
    "        container,\n",
    "        labels=labels,\n",
    "        label_type='center',  # Text in the middle of the bar\n",
    "        padding=0,\n",
    "        fontsize=10,\n",
    "        color='white',  # White text\n",
    "        weight='bold'  # Bold for readability\n",
    "    )\n",
    "\n",
    "\n",
    "# Remove top and right spines\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "# Important: adjust layout so the legend is not cut off when saving\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "\n",
    "# Save\n",
    "output_path = \"metrics_by_threshold__rf.png\"\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64abf67",
   "metadata": {},
   "source": [
    "## XGB Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380ddffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost has different parameters than Random Forest.\n",
    "# The key one is `scale_pos_weight`, which helps \"force\" higher Recall (important for F2).\n",
    "search_spaces_xgb = {\n",
    "    'n_estimators': Integer(100, 500),\n",
    "    'max_depth': Integer(3, 10),  # Boosting prefers shallower trees than RF\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'subsample': Real(0.6, 1.0),  # Prevents overfitting\n",
    "    'colsample_bytree': Real(0.6, 1.0),  # Equivalent of max_features\n",
    "    'scale_pos_weight': Integer(1, 6)  # MAGIC FOR F2: gives the positive class more weight\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06db3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Starting {N_FOLDS}-fold cross-validation...\")\n",
    "print(f\"Testing thresholds: {THRESHOLDS}\")\n",
    "print(f\"Sample size: {len(y_sampled)} | Train per fold: {int(len(y_sampled) * (N_FOLDS - 1) / N_FOLDS)}\")\n",
    "\n",
    "\n",
    "# Stratified K-Fold zapewnia, Å¼e w kaÅ¼dym \"foldzie\" proporcja klas jest zachowana\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "# F2\n",
    "beta = 2\n",
    "\n",
    "# Listy na wyniki\n",
    "results_by_threshold = {t: {\n",
    "    \"val_f2\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f2\": []\n",
    "} for t in THRESHOLDS}\n",
    "\n",
    "best_params_history = []\n",
    "\n",
    "\n",
    "fold_no = 1\n",
    "for train_idx_local, test_idx_local in skf.split(X_sampled, y_sampled):\n",
    "    # --- 1) Local train/test within the sampled subset\n",
    "    X_train_fold = X_sampled[train_idx_local]\n",
    "    y_train_fold = y_sampled[train_idx_local]\n",
    "    X_test_fold = X_sampled[test_idx_local]\n",
    "    y_test_fold = y_sampled[test_idx_local]\n",
    "\n",
    "    # Global test pool = all rows NOT in fold train set\n",
    "    global_train_idx = sample_idx[train_idx_local]\n",
    "    mask_train = np.zeros(len(df_merged), dtype=bool)\n",
    "    mask_train[global_train_idx] = True\n",
    "    X_test_global = X_full[~mask_train]\n",
    "\n",
    "    # --- 2) Bayesian hyperparameter search\n",
    "    opt = BayesSearchCV(\n",
    "        estimator=XGBClassifier(\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_SEED,\n",
    "            eval_metric='logloss', \n",
    "            use_label_encoder=False\n",
    "        ),\n",
    "        search_spaces=rf_search_space,\n",
    "        scoring='f1',\n",
    "        cv=3,\n",
    "        n_iter=15,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "    opt.fit(X_train_fold, y_train_fold)\n",
    "    best_model_xgb = opt.best_estimator_\n",
    "    best_params_xgb = dict(opt.best_params_)\n",
    "    best_params_history.append(best_params_xgb)\n",
    "    print(f\"\\nFold {fold_no}: Params: {best_params_xgb}\")\n",
    "\n",
    "    # --- 3) Predict probabilities once for both local and global test sets\n",
    "    probs_local = best_model_xgb.predict_proba(X_test_fold)[:, 1]\n",
    "    probs_global = best_model_xgb.predict_proba(X_test_global)[:, 1]\n",
    "\n",
    "    # --- 4) Filter gold set to fold-specific positives (leakage control)\n",
    "    train_positive_indices = global_train_idx[y_train_fold == 1]\n",
    "    train_positive_merge_cols = df_merged.iloc[train_positive_indices][\"merge_col\"]\n",
    "    df_res_filtered = df_gold[~df_gold.merge_col.isin(train_positive_merge_cols)]\n",
    "\n",
    "    # --- 5) Loop over thresholds and evaluate\n",
    "    best_fold_stats = {\"f2_global\": -1, \"thr\": 0, \"p\": 0, \"r\": 0, \"val_f2\": 0}\n",
    "    for t in THRESHOLDS:\n",
    "        # Local validation metrics\n",
    "        preds_local_t = (probs_local >= t).astype(int)\n",
    "        val_f2 = fbeta_score(y_test_fold, preds_local_t, beta=2, zero_division=0)\n",
    "        val_acc = accuracy_score(y_test_fold, preds_local_t)\n",
    "\n",
    "        # Global prediction pool\n",
    "        val_preds_global = (probs_global >= t).astype(int)\n",
    "\n",
    "        # Set-based metrics\n",
    "        df_test_fold = df_merged[~mask_train].copy()\n",
    "        df_test_fold[\"result\"] = val_preds_global\n",
    "\n",
    "        res_filt = df_test_fold[df_test_fold[\"result\"] == 1]\n",
    "        org_set = set(df_res_filtered[[\"polish_label\", \"name_variant\"]].itertuples(index=False, name=None))\n",
    "        new_set = set(res_filt[[\"polish_label\", \"name_variant\"]].itertuples(index=False, name=None))\n",
    "\n",
    "        true_positive_count = len(new_set & org_set)\n",
    "        false_positive_count = len(new_set - org_set)\n",
    "        false_negative_df = df_res_filtered.loc[~df_res_filtered[\"merge_col\"].isin(res_filt[\"merge_col\"])]\n",
    "        false_negative_count = len(false_negative_df)\n",
    "\n",
    "        prec = true_positive_count / (true_positive_count + false_positive_count) if (\n",
    "                                                                                             true_positive_count + false_positive_count) > 0 else 0\n",
    "        rec = true_positive_count / (true_positive_count + false_negative_count) if (\n",
    "                                                                                            true_positive_count + false_negative_count) > 0 else 0\n",
    "        f2_global = (1 + beta ** 2) * (prec * rec) / (beta ** 2 * prec + rec) if (beta ** 2 * prec + rec) > 0 else 0\n",
    "\n",
    "        # Store results for this threshold across folds\n",
    "        results_by_threshold[t][\"val_f2\"].append(val_f2)\n",
    "        results_by_threshold[t][\"accuracy\"].append(val_acc)\n",
    "        results_by_threshold[t][\"precision\"].append(prec)\n",
    "        results_by_threshold[t][\"recall\"].append(rec)\n",
    "        results_by_threshold[t][\"f2\"].append(f2_global)\n",
    "\n",
    "        # Update best threshold for this fold\n",
    "        if f2_global > best_fold_stats[\"f2_global\"]:\n",
    "            best_fold_stats = {\n",
    "                \"f2_global\": f2_global,\n",
    "                \"thr\": t,\n",
    "                \"p\": prec,\n",
    "                \"r\": rec,\n",
    "                \"val_f2\": val_f2\n",
    "            }\n",
    "\n",
    "    # Print per fold\n",
    "    print(\n",
    "        f\"Fold {fold_no}: Best Thr: {best_fold_stats['thr']:.2f} | Val F2: {best_fold_stats['val_f2']:.4f} | Global F2: {best_fold_stats['f2_global']:.4f} (P: {best_fold_stats['p']:.2f}, R: {best_fold_stats['r']:.2f})\")\n",
    "    fold_no += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c9119",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SUMMARY OF PARAMETERS (Mode / Most Frequent) ---\n",
    "params_df = pd.DataFrame(best_params_history)\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"HYPERPARAMETER ANALYSIS (Best Params from 5 folds)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Compute the most frequent value (mode) for each parameter\n",
    "most_common_params = params_df.mode().iloc[0].to_dict()\n",
    "print(\"Most frequent configuration (Mode):\")\n",
    "for k, v in most_common_params.items():\n",
    "    # If parameters are numbers, format nicely; if strings, leave as-is\n",
    "    val_str = f\"{int(v)}\" if isinstance(v, (int, np.integer)) else f\"{v}\"\n",
    "    print(f\"  {k}: {val_str}\")\n",
    "\n",
    "\n",
    "# --- SUMMARY OF RESULTS FOR EACH THRESHOLD ---\n",
    "# 1) First, find the winning threshold (highest average F2)\n",
    "best_avg_f2 = -1\n",
    "best_t_final = None\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    current_avg_f2 = np.mean(results_by_threshold[t]['f2'])\n",
    "    if current_avg_f2 > best_avg_f2:\n",
    "        best_avg_f2 = current_avg_f2\n",
    "        best_t_final = t\n",
    "\n",
    "\n",
    "# 2) Print results, marking the winner in the header\n",
    "for t in THRESHOLDS:\n",
    "    res = results_by_threshold[t]\n",
    "\n",
    "    # Check if this is the winner\n",
    "    if t == best_t_final:\n",
    "        header_suffix = \" ðŸ† \"\n",
    "    else:\n",
    "        header_suffix = \"\"\n",
    "\n",
    "    print(f\"\\n=== {header_suffix} FINAL RESULTS (Mean +/- Std) for Threshold: {t:.2f} ===\")\n",
    "    print(f\"Val F2 (Local): {np.mean(res['val_f2']):.4f} Â± {np.std(res['val_f2']):.4f}\")\n",
    "    print(f\"Accuracy (Val): {np.mean(res['accuracy']):.4f} Â± {np.std(res['accuracy']):.4f}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Precision:      {np.mean(res['precision']):.4f} Â± {np.std(res['precision']):.4f}\")\n",
    "    print(f\"Recall:         {np.mean(res['recall']):.4f} Â± {np.std(res['recall']):.4f}\")\n",
    "    print(f\"F2-Score:       {np.mean(res['f2']):.4f} Â± {np.std(res['f2']):.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert the nested dictionary `results_by_threshold` into a flat DataFrame\n",
    "data_rows = []\n",
    "\n",
    "for t, metrics in results_by_threshold.items():\n",
    "    # `metrics` is a dictionary of lists, e.g. [0.7, 0.72, ...] for each fold\n",
    "    n_folds_actual = len(metrics['f2'])  # should be 5\n",
    "\n",
    "    for i in range(n_folds_actual):\n",
    "        # Add rows for each metric we care about\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Global F2', 'Score': metrics['f2'][i]})\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Precision', 'Score': metrics['precision'][i]})\n",
    "        data_rows.append({'Threshold': t, 'Fold': i + 1, 'Metric': 'Recall', 'Score': metrics['recall'][i]})\n",
    "        # Optionally add Val F2 if you want to compare it\n",
    "        # data_rows.append({'Threshold': t, 'Fold': i+1, 'Metric': 'Local Val F2', 'Score': metrics['val_f2'][i]})\n",
    "\n",
    "df_metrics = pd.DataFrame(data_rows)\n",
    "\n",
    "# Let's see what it looks like\n",
    "# df_metrics\n",
    "\n",
    "\n",
    "# 2. Style configuration\n",
    "sns.set_theme(style=\"whitegrid\", context=\"paper\", font_scale=1.2)\n",
    "plt.figure(figsize=(10, 7))  # Slightly taller chart to fit the legend at the bottom\n",
    "\n",
    "\n",
    "# 3. Define colors (Manual for full controlâ€”matching your screenshot)\n",
    "# Global F2 (Blue), Precision (Orange), Recall (Green)\n",
    "colors = sns.color_palette(\"deep\", 3)\n",
    "\n",
    "\n",
    "# 4. Draw the plot\n",
    "ax = sns.barplot(\n",
    "    data=df_metrics,\n",
    "    x=\"Threshold\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Metric\",\n",
    "    palette=colors,  # Use the \"deep\" palette (as in the screenshot) instead of viridis\n",
    "    capsize=.05,  # Slightly more subtle error bar caps\n",
    "    errorbar=\"sd\",\n",
    "    edgecolor=\"0.3\",  # Dark gray border for contrast\n",
    "    linewidth=1\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Titles and axes\n",
    "plt.title('Impact of Threshold on Model Performance (5-Fold CV)', fontsize=16, weight='bold', y=1.02)\n",
    "plt.xlabel('Decision Threshold', fontsize=13, labelpad=10)\n",
    "plt.ylabel('Score (0-1)', fontsize=13, labelpad=10)\n",
    "plt.ylim(0, 1.1)\n",
    "\n",
    "\n",
    "# 6. IMPROVED LEGEND (Placed below the plot)\n",
    "plt.legend(\n",
    "    title=None,  # Remove the unnecessary \"Metric\" title\n",
    "    loc='upper center',  # Legend anchor point\n",
    "    bbox_to_anchor=(0.5, -0.15),  # Shift below the X axis\n",
    "    ncol=3,  # All items on one line\n",
    "    frameon=False,  # No border/frame\n",
    "    fontsize=12\n",
    ")\n",
    "\n",
    "\n",
    "# 7. Values on bars (Annotation)\n",
    "for container in ax.containers:\n",
    "    # Get bar heights and format them\n",
    "    labels = [f'{v.get_height():.2f}' for v in container]\n",
    "\n",
    "    ax.bar_label(\n",
    "        container,\n",
    "        labels=labels,\n",
    "        label_type='center',  # Text in the middle of the bar\n",
    "        padding=0,\n",
    "        fontsize=10,\n",
    "        color='white',  # White text\n",
    "        weight='bold'  # Bold for readability\n",
    "    )\n",
    "\n",
    "\n",
    "# Remove top and right spines\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "# Important: adjust layout so the legend is not cut off when saving\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "\n",
    "\n",
    "# Save\n",
    "output_path = \"metrics_by_threshold__xgb.png\"\n",
    "plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6b8fc",
   "metadata": {},
   "source": [
    "# ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca047348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def plot_roc_curve(\n",
    "        model: RandomForestClassifier,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        title: str = \"ROC curve (Random Forest)\",\n",
    "        save_path: Path | None = None,\n",
    "        mark_threshold: float | None = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Plot ROC curve for a binary classifier and optionally save to file.\n",
    "\n",
    "    Returns fpr, tpr, thresholds, auc_value.\n",
    "    \"\"\"\n",
    "    proba = model.predict_proba(X)[:, 1]\n",
    "    fpr, tpr, thresholds = roc_curve(y, proba)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.0, 4.0), dpi=300)\n",
    "\n",
    "    # ROC curve\n",
    "    ax.plot(fpr, tpr, lw=1.8, label=f\"AUC = {auc_value:.3f}\")\n",
    "\n",
    "    # Diagonal (random classifier)\n",
    "    ax.plot([0.0, 1.0], [0.0, 1.0], \"--\", lw=1.0, label=\"Random\")\n",
    "\n",
    "    # Optional: mark chosen operating point (e.g. best_t_final = 0.45)\n",
    "    if mark_threshold is not None:\n",
    "        idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "        ax.scatter(\n",
    "            fpr[idx],\n",
    "            tpr[idx],\n",
    "            s=25,\n",
    "            marker=\"o\",\n",
    "            zorder=3,\n",
    "            label=f\"Threshold = {mark_threshold:.2f}\",\n",
    "        )\n",
    "\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.02)\n",
    "    ax.set_xlabel(\"False positive rate\")\n",
    "    ax.set_ylabel(\"True positive rate\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.grid(True, linestyle=\":\", linewidth=0.5)\n",
    "    ax.legend(loc=\"lower right\", frameon=True)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "\n",
    "    return fpr, tpr, thresholds, auc_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4840e2a7",
   "metadata": {},
   "source": [
    "# ROC RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path =  Path(\"figures/roc_rf_similarity.pdf\")\n",
    "\n",
    "best_t_final = 0.45 # or whatever was the best threshold from the code\n",
    "\n",
    "fpr, tpr, thresholds, auc_value = plot_roc_curve(\n",
    "    model=best_model_rf,\n",
    "    X=X_sampled,\n",
    "    y=y_sampled,\n",
    "    title=\"Random Forest - ROC curve\",\n",
    "    save_path=fig_path,\n",
    "    mark_threshold=best_t_final,\n",
    ")\n",
    "\n",
    "print(f\"AUC: {auc_value:.3f}\")\n",
    "print(f\"Wykres zapisany do: {fig_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f83868",
   "metadata": {},
   "source": [
    "# ROC XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b605a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_path =  Path(\"figures/roc_xgb_similarity.pdf\")\n",
    "\n",
    "best_t_final = 0.45 # or whatever was the best threshold from the code\n",
    "\n",
    "fpr, tpr, thresholds, auc_value = plot_roc_curve(\n",
    "    model=best_model_xgb,\n",
    "    X=X_sampled,\n",
    "    y=y_sampled,\n",
    "    title=\"XGBoost - ROC curve\",\n",
    "    save_path=fig_path,\n",
    "    mark_threshold=best_t_final,\n",
    ")\n",
    "\n",
    "print(f\"AUC: {auc_value:.3f}\")\n",
    "print(f\"Wykres zapisany do: {fig_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9146ac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "er_venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
