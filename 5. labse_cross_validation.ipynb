{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01556f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.metrics import fbeta_score, accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, util\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "N_FOLDS = 5\n",
    "EPOCHS=3\n",
    "THRESHOLDS = [0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]  # Thresholds to test\n",
    "\n",
    "FULL_TEST = False\n",
    "\n",
    "N_SAMPLES_PER_CLASS = 500\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241f982d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "objects_to_delete = ['model', 'train_loss', 'evaluator', 'train_dataloader', 'trainer']\n",
    "\n",
    "for obj in objects_to_delete:\n",
    "    if obj in globals():\n",
    "        del globals()[obj]\n",
    "\n",
    "# 2. Force Python garbage collection (frees CPU RAM)\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clear the PyTorch allocator cache (frees GPU memory)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 4. Check available memory\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c51d05",
   "metadata": {},
   "source": [
    "\n",
    "# --- 1. Data preparation ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet(\"df_merged.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17a5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the columns that determine whether a row is a duplicate\n",
    "# For the LaBSE model, the uniqueness of the input text pair is key.\n",
    "duplicate_subset = ['polish_label', 'name_variant']\n",
    "\n",
    "\n",
    "# 3. Remove duplicates\n",
    "# keep='first' keeps the first occurrence and drops subsequent repeats.\n",
    "# inplace=False (default) returns a new DataFrame.\n",
    "df_full_clean = df_full.drop_duplicates(subset=duplicate_subset, keep='first')\n",
    "\n",
    "\n",
    "# 4. Reset the index (VERY IMPORTANT for KFold)\n",
    "# drop=True removes the old 'index' column so it doesn't clutter the DataFrame.\n",
    "df_full_clean = df_full_clean.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(f\"Number of rows after removing duplicates: {len(df_full_clean)}\")\n",
    "print(f\"Removed {len(df_full) - len(df_full_clean)} duplicates.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if FULL_TEST:\n",
    "    # 1. Full test (dtandard 85% / 15%)\n",
    "    df_cv_pool, df_oos = train_test_split(df_full_clean, test_size=0.15, random_state=RANDOM_SEED, stratify=df_full_clean['truth'])\n",
    "else:\n",
    "    # retrieve same train / test split as before\n",
    "    y_full = df_full_clean[\"truth\"].astype(int).values\n",
    "    feature_cols = [c for c in df_full.columns if c.startswith((\"coverage\", \"len\"))]\n",
    "\n",
    "    rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "    pos_idx = np.where(y_full == 1)[0]\n",
    "    neg_idx = np.where(y_full == 0)[0]\n",
    "    n = min(N_SAMPLES_PER_CLASS, len(pos_idx), len(neg_idx))\n",
    "\n",
    "    sample_idx = np.concatenate([\n",
    "        rng.choice(pos_idx, size=n, replace=False),\n",
    "        rng.choice(neg_idx, size=n, replace=False),\n",
    "    ])\n",
    "\n",
    "\n",
    "    mask = ~np.isin(np.arange(len(df_full_clean)), sample_idx)\n",
    "    df_not_in_sample = df_full_clean[mask]\n",
    "\n",
    "    df_oos = df_full_clean.loc[~df_full_clean.index.isin(sample_idx)]\n",
    "    df_cv_pool = df_full_clean.loc[df_full_clean.index.isin(sample_idx)]\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Dane do Cross-Validation: {len(df_cv_pool)}\")\n",
    "print(f\"Dane do OOS Evaluation: {len(df_oos)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, df, thresholds):\n",
    "    \"\"\"\n",
    "    Computes metrics for a given model and DataFrame across a list of thresholds.\n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    sentences1 = df['polish_label'].tolist()\n",
    "    sentences2 = df['name_variant'].tolist()\n",
    "    gt_labels = [1 if x else 0 for x in df['truth'].tolist()]  # Assumes `truth` is bool or 0/1\n",
    "\n",
    "    # Encode (use batching for speed)\n",
    "    embs1 = model.encode(sentences1, convert_to_tensor=True, show_progress_bar=False)\n",
    "    embs2 = model.encode(sentences2, convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "    # Cosine similarity\n",
    "    cosine_scores = util.cos_sim(embs1, embs2).diagonal().cpu().numpy()\n",
    "\n",
    "    results = {}\n",
    "    for t in thresholds:\n",
    "        pred_labels = (cosine_scores >= t).astype(int)\n",
    "\n",
    "        # Compute metrics\n",
    "        f2 = fbeta_score(gt_labels, pred_labels, beta=2, zero_division=0)\n",
    "        acc = accuracy_score(gt_labels, pred_labels)\n",
    "        prec = precision_score(gt_labels, pred_labels, zero_division=0)\n",
    "        rec = recall_score(gt_labels, pred_labels, zero_division=0)\n",
    "\n",
    "        results[t] = {\n",
    "            'f2': f2,\n",
    "            'acc': acc,\n",
    "            'prec': prec,\n",
    "            'rec': rec\n",
    "        }\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b6ec7f",
   "metadata": {},
   "source": [
    "# --- 2. MAIN CROSS-VALIDATION LOOP ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Structure to store results from each fold\n",
    "# fold_results = [\n",
    "#    { 0.35: {'val_f2': ..., 'oos_f2': ...}, 0.40: {...} },  <-- Fold 1\n",
    "#    { ... }                                                 <-- Fold 2\n",
    "# ]\n",
    "all_folds_data = []\n",
    "\n",
    "final_df_cv_pool = df_cv_pool\n",
    "\n",
    "\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(final_df_cv_pool)):\n",
    "    print(f\"\\n--- FOLD {fold_idx+1}/{N_FOLDS} ---\")\n",
    "\n",
    "    # Split the data\n",
    "    train_df = final_df_cv_pool.iloc[train_idx]\n",
    "    val_df = final_df_cv_pool.iloc[val_idx]\n",
    "\n",
    "    # Prepare training\n",
    "    train_examples = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        label = 0.9 if row['truth'] else 0.1  # Soft labels for LaBSE\n",
    "        train_examples.append(InputExample(texts=[row['polish_label'], row['name_variant']], label=label))\n",
    "    print(len(train_examples))\n",
    "\n",
    "    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=32)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = SentenceTransformer(\"sentence-transformers/LaBSE\")\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    # Training (short, e.g., 1–3 epochs)\n",
    "    model.fit(\n",
    "        train_objectives=[(train_dataloader, train_loss)],\n",
    "        epochs=EPOCHS,\n",
    "        warmup_steps=100,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "\n",
    "    # --- EVALUATION ---\n",
    "    print(\"Evaluating Validation subset...\")\n",
    "    val_metrics = compute_metrics(model, val_df, THRESHOLDS)\n",
    "\n",
    "    print(\"Evaluating OOS pool...\")\n",
    "    oos_metrics = compute_metrics(model, df_oos, THRESHOLDS)\n",
    "\n",
    "    # Collect results for this fold\n",
    "    fold_data = {}\n",
    "    for t in THRESHOLDS:\n",
    "        fold_data[t] = {\n",
    "            # Validation subset (per your screenshot: F2, Accuracy)\n",
    "            'val_f2': val_metrics[t]['f2'],\n",
    "            'val_acc': val_metrics[t]['acc'],\n",
    "\n",
    "            # OOS pool (per screenshot: Precision, Recall, F2)\n",
    "            'oos_prec': oos_metrics[t]['prec'],\n",
    "            'oos_rec': oos_metrics[t]['rec'],\n",
    "            'oos_f2': oos_metrics[t]['f2']\n",
    "        }\n",
    "    all_folds_data.append(fold_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a21485",
   "metadata": {},
   "source": [
    "# --- 3. RESULTS AGGERGATION AND TABLE FORMATION ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b075c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = []\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    # Extract results for this threshold from all folds\n",
    "    metrics_at_t = [fold[t] for fold in all_folds_data]\n",
    "\n",
    "    # Convert to a DataFrame for easy mean/std computation\n",
    "    df_metrics = pd.DataFrame(metrics_at_t)\n",
    "\n",
    "    row = {'Threshold': t}\n",
    "\n",
    "    # For each column, compute \"mean ± std\"\n",
    "    for col in df_metrics.columns:\n",
    "        mean = df_metrics[col].mean()\n",
    "        std = df_metrics[col].std()\n",
    "        row[col] = f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "    final_summary.append(row)\n",
    "\n",
    "df_final_table = pd.DataFrame(final_summary)\n",
    "df_final_table.set_index('Threshold', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf4ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display (transposed so it looks like the screenshot, if you prefer)\n",
    "print(\"\\n=== FINAL CROSS-VALIDATION TABLE ===\")\n",
    "display(df_final_table.T)  # .T puts metrics in rows and thresholds in columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452d16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_TEST:\n",
    "    filename_base = \"labse_big_finetune_results\"\n",
    "else:\n",
    "    filename_base = \"labse_smaller_finetune_results\"\n",
    "\n",
    "\n",
    "# 1. Save to CSV (for easy inspection)\n",
    "csv_path = f\"output/{filename_base}.csv\"\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "df_final_table.to_csv(csv_path)\n",
    "print(f\"Saved the table (CSV) to: {csv_path}\")\n",
    "\n",
    "\n",
    "# 2. Save to Parquet (to preserve precision and data types)\n",
    "parquet_path = f\"output/{filename_base}.parquet\"\n",
    "df_final_table.to_parquet(parquet_path)\n",
    "print(f\"Saved the table (Parquet) to: {parquet_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e8361",
   "metadata": {},
   "source": [
    "# Export LATEX results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9fc3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_latex_table(\n",
    "    df_results: pd.DataFrame, \n",
    "    num_folds: int, \n",
    "    model_name: str, \n",
    "    selected_thresholds: Optional[List[float]] = None,\n",
    "    label_suffix: str = \"model\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates and prints a LaTeX table code snippet from a results DataFrame, \n",
    "    formatted specifically for academic papers. Allows filtering by specific thresholds.\n",
    "\n",
    "    Args:\n",
    "        df_results (pd.DataFrame): DataFrame containing result strings (e.g., \"0.850 ± 0.010\").\n",
    "                                   Expected index: Thresholds (float).\n",
    "        num_folds (int): Number of folds used in cross-validation.\n",
    "        model_name (str): Name of the model.\n",
    "        selected_thresholds (List[float], optional): List of thresholds to include in the table.\n",
    "                                                     If None, all thresholds from df_results are used.\n",
    "        label_suffix (str): Suffix for the LaTeX label.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the selected_thresholds are not present in df_results index.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter Thresholds if specific list provided\n",
    "    if selected_thresholds is not None:\n",
    "        # Check for missing thresholds\n",
    "        missing = [t for t in selected_thresholds if t not in df_results.index]\n",
    "        if missing:\n",
    "            available = df_results.index.tolist()\n",
    "            raise ValueError(\n",
    "                f\"Thresholds {missing} not found in results.\\n\"\n",
    "                f\"Available thresholds are: {available}\"\n",
    "            )\n",
    "        \n",
    "        # Filter the DataFrame ensuring the order matches selected_thresholds\n",
    "        df_filtered = df_results.loc[selected_thresholds]\n",
    "    else:\n",
    "        df_filtered = df_results\n",
    "\n",
    "    # 2. Transpose: Thresholds become columns, Metrics become rows\n",
    "    df_T = df_filtered.T\n",
    "    \n",
    "    # 3. Define Metric Mapping\n",
    "    metric_map = {\n",
    "        'val_f2': 'F$_2$ score',\n",
    "        'val_acc': 'Accuracy',\n",
    "        'oos_prec': 'Precision',\n",
    "        'oos_rec': 'Recall',\n",
    "        'oos_f2': 'F$_2$ score'\n",
    "    }\n",
    "    \n",
    "    # 4. Helper function to format a row\n",
    "    def format_row_content(metric_key, row_series):\n",
    "        latex_cells = []\n",
    "        \n",
    "        # Parse values to find max for bolding (only among selected thresholds)\n",
    "        values_dict = {}\n",
    "        for col_idx, val_str in row_series.items():\n",
    "            clean_val = str(val_str).replace('±', '').replace('$\\pm$', '')\n",
    "            try:\n",
    "                mean_val = float(clean_val.split()[0])\n",
    "                values_dict[col_idx] = mean_val\n",
    "            except (ValueError, IndexError):\n",
    "                values_dict[col_idx] = -1.0\n",
    "\n",
    "        max_val = max(values_dict.values()) if values_dict else 0\n",
    "        \n",
    "        for col_idx, val_str in row_series.items():\n",
    "            formatted_str = str(val_str).replace('±', r'$\\pm$')\n",
    "            \n",
    "            # Bold if it's the max value in this specific view\n",
    "            if values_dict.get(col_idx) == max_val and max_val > 0:\n",
    "                formatted_str = f\"\\\\textbf{{{formatted_str}}}\"\n",
    "            \n",
    "            latex_cells.append(formatted_str)\n",
    "            \n",
    "        return f\"{metric_map.get(metric_key, metric_key)} & \" + \" & \".join(latex_cells) + \" \\\\\\\\\"\n",
    "\n",
    "    # 5. Prepare Headers\n",
    "    thresholds = df_T.columns.tolist()\n",
    "    header_cols = \" & \".join([f\"{t:.2f}\" for t in thresholds])\n",
    "    col_def = \"c\" * len(thresholds)\n",
    "    \n",
    "    # 6. Build LaTeX\n",
    "    latex_code = [\n",
    "        r\"\\begin{table*}[!ht]\",\n",
    "        r\"    \\centering\",\n",
    "        f\"    \\\\caption{{{model_name}: cross-validated performance \\\\\\\\for different decision thresholds (mean $\\\\pm$ standard deviation over {num_folds} folds).}}\",\n",
    "        f\"    \\\\label{{tab:{label_suffix}_thresholds}}\",\n",
    "        f\"    \\\\begin{{tabular}}{{l{col_def}}}\",\n",
    "        r\"        \\hline\",\n",
    "        f\"        & \\\\multicolumn{{{len(thresholds)}}}{{c}}{{Decision threshold $\\\\tau$}} \\\\\\\\\",\n",
    "        f\"        \\\\cline{{2-{len(thresholds)+1}}}\",\n",
    "        f\"        Metric & {header_cols} \\\\\\\\\",\n",
    "        r\"        \\hline\",\n",
    "        r\"        \\multicolumn{\" + str(len(thresholds)+1) + r\"}{l}{\\textit{Validation subset}} \\\\\"\n",
    "    ]\n",
    "\n",
    "    for metric in ['val_f2', 'val_acc']:\n",
    "        if metric in df_T.index:\n",
    "            latex_code.append(\"        \" + format_row_content(metric, df_T.loc[metric]))\n",
    "\n",
    "    latex_code.append(r\"        \\hline\")\n",
    "    latex_code.append(r\"        \\multicolumn{\" + str(len(thresholds)+1) + r\"}{l}{\\textit{Out-of-sample evaluation pool}} \\\\\")\n",
    "\n",
    "    for metric in ['oos_prec', 'oos_rec', 'oos_f2']:\n",
    "        if metric in df_T.index:\n",
    "            latex_code.append(\"        \" + format_row_content(metric, df_T.loc[metric]))\n",
    "\n",
    "    latex_code.extend([\n",
    "        r\"        \\hline\",\n",
    "        r\"    \\end{tabular}\",\n",
    "        r\"\\end{table*}\"\n",
    "    ])\n",
    "\n",
    "    print(\"\\n\".join(latex_code))\n",
    "\n",
    "def export_latex_table(\n",
    "    df_results: pd.DataFrame, \n",
    "    num_folds: int, \n",
    "    model_name: str, \n",
    "    selected_thresholds: Optional[List[float]] = None,\n",
    "    label_suffix: str = \"model\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates and prints a LaTeX table code snippet from a results DataFrame, \n",
    "    formatted specifically for academic papers. Allows filtering by specific thresholds.\n",
    "\n",
    "    Args:\n",
    "        df_results (pd.DataFrame): DataFrame containing result strings (e.g., \"0.850 ± 0.010\").\n",
    "                                   Expected index: Thresholds (float).\n",
    "        num_folds (int): Number of folds used in cross-validation.\n",
    "        model_name (str): Name of the model.\n",
    "        selected_thresholds (List[float], optional): List of thresholds to include in the table.\n",
    "                                                     If None, all thresholds from df_results are used.\n",
    "        label_suffix (str): Suffix for the LaTeX label.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If any of the selected_thresholds are not present in df_results index.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter Thresholds if specific list provided\n",
    "    if selected_thresholds is not None:\n",
    "        # Check for missing thresholds\n",
    "        missing = [t for t in selected_thresholds if t not in df_results.index]\n",
    "        if missing:\n",
    "            available = df_results.index.tolist()\n",
    "            raise ValueError(\n",
    "                f\"Thresholds {missing} not found in results.\\n\"\n",
    "                f\"Available thresholds are: {available}\"\n",
    "            )\n",
    "        \n",
    "        # Filter the DataFrame ensuring the order matches selected_thresholds\n",
    "        df_filtered = df_results.loc[selected_thresholds]\n",
    "    else:\n",
    "        df_filtered = df_results\n",
    "\n",
    "    # 2. Transpose: Thresholds become columns, Metrics become rows\n",
    "    df_T = df_filtered.T\n",
    "    \n",
    "    # 3. Define Metric Mapping\n",
    "    metric_map = {\n",
    "        'val_f2': 'F$_2$ score',\n",
    "        'val_acc': 'Accuracy',\n",
    "        'oos_prec': 'Precision',\n",
    "        'oos_rec': 'Recall',\n",
    "        'oos_f2': 'F$_2$ score'\n",
    "    }\n",
    "    \n",
    "    # 4. Helper function to format a row\n",
    "    def format_row_content(metric_key, row_series):\n",
    "        latex_cells = []\n",
    "        \n",
    "        # Parse values to find max for bolding (only among selected thresholds)\n",
    "        values_dict = {}\n",
    "        for col_idx, val_str in row_series.items():\n",
    "            clean_val = str(val_str).replace('±', '').replace('$\\pm$', '')\n",
    "            try:\n",
    "                mean_val = float(clean_val.split()[0])\n",
    "                values_dict[col_idx] = mean_val\n",
    "            except (ValueError, IndexError):\n",
    "                values_dict[col_idx] = -1.0\n",
    "\n",
    "        max_val = max(values_dict.values()) if values_dict else 0\n",
    "        \n",
    "        for col_idx, val_str in row_series.items():\n",
    "            formatted_str = str(val_str).replace('±', r'$\\pm$')\n",
    "            \n",
    "            # Bold if it's the max value in this specific view\n",
    "            if values_dict.get(col_idx) == max_val and max_val > 0:\n",
    "                formatted_str = f\"\\\\textbf{{{formatted_str}}}\"\n",
    "            \n",
    "            latex_cells.append(formatted_str)\n",
    "            \n",
    "        return f\"{metric_map.get(metric_key, metric_key)} & \" + \" & \".join(latex_cells) + \" \\\\\\\\\"\n",
    "\n",
    "    # 5. Prepare Headers\n",
    "    thresholds = df_T.columns.tolist()\n",
    "    header_cols = \" & \".join([f\"{t:.2f}\" for t in thresholds])\n",
    "    col_def = \"c\" * len(thresholds)\n",
    "    \n",
    "    # 6. Build LaTeX\n",
    "    latex_code = [\n",
    "        r\"\\begin{table*}[!ht]\",\n",
    "        r\"    \\centering\",\n",
    "        f\"    \\\\caption{{{model_name}: cross-validated performance \\\\\\\\for different decision thresholds (mean $\\\\pm$ standard deviation over {num_folds} folds).}}\",\n",
    "        f\"    \\\\label{{tab:{label_suffix}_thresholds}}\",\n",
    "        f\"    \\\\begin{{tabular}}{{l{col_def}}}\",\n",
    "        r\"        \\hline\",\n",
    "        f\"        & \\\\multicolumn{{{len(thresholds)}}}{{c}}{{Decision threshold $\\\\tau$}} \\\\\\\\\",\n",
    "        f\"        \\\\cline{{2-{len(thresholds)+1}}}\",\n",
    "        f\"        Metric & {header_cols} \\\\\\\\\",\n",
    "        r\"        \\hline\",\n",
    "        r\"        \\multicolumn{\" + str(len(thresholds)+1) + r\"}{l}{\\textit{Validation subset}} \\\\\"\n",
    "    ]\n",
    "\n",
    "    for metric in ['val_f2', 'val_acc']:\n",
    "        if metric in df_T.index:\n",
    "            latex_code.append(\"        \" + format_row_content(metric, df_T.loc[metric]))\n",
    "\n",
    "    latex_code.append(r\"        \\hline\")\n",
    "    latex_code.append(r\"        \\multicolumn{\" + str(len(thresholds)+1) + r\"}{l}{\\textit{Out-of-sample evaluation pool}} \\\\\")\n",
    "\n",
    "    for metric in ['oos_prec', 'oos_rec', 'oos_f2']:\n",
    "        if metric in df_T.index:\n",
    "            latex_code.append(\"        \" + format_row_content(metric, df_T.loc[metric]))\n",
    "\n",
    "    latex_code.extend([\n",
    "        r\"        \\hline\",\n",
    "        r\"    \\end{tabular}\",\n",
    "        r\"\\end{table*}\"\n",
    "    ])\n",
    "\n",
    "    print(\"\\n\".join(latex_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503469f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_latex_table(\n",
    "    df_results=df_final_table, \n",
    "    num_folds=N_FOLDS, \n",
    "    model_name=\"LaBSE\", \n",
    "    label_suffix=\"labse_smaller_finetune\",\n",
    "    selected_thresholds=[0.35, 0.4, 0.45, 0.5, ] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b9a0b",
   "metadata": {},
   "source": [
    "# ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5d01a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Prepare the data (e.g., from the OOS Evaluation Pool)\n",
    "df_eval = df_oos.copy()  # We use your \"Evaluation Pool\" dataset\n",
    "\n",
    "\n",
    "# 2. Generate embeddings\n",
    "embs1 = model.encode(df_eval['polish_label'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "embs2 = model.encode(df_eval['name_variant'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "\n",
    "# 3. Compute similarity (these are your y_scores!)\n",
    "# util.cos_sim returns a matrix; we take the diagonal (pairs aligned row-by-row)\n",
    "cosine_scores = util.cos_sim(embs1, embs2).diagonal().cpu().numpy()\n",
    "\n",
    "\n",
    "# 4. Prepare labels (ground truth)\n",
    "y_true = df_eval['truth'].astype(int).to_numpy()  # Make sure these are 0 and 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f56ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve_scores(\n",
    "        y_true: np.ndarray,\n",
    "        y_scores: np.ndarray,\n",
    "        title: str = \"ROC curve\",\n",
    "        save_path: Optional[Path] = None,\n",
    "        mark_threshold: Optional[float] = None,\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Plot an ROC curve based on pre-computed scores (e.g., cosine similarity)\n",
    "    and optionally save it to a file. Keeps exact visual consistency with previous plots.\n",
    "    Uses Seaborn's 'darkgrid' style to exactly match the reference chart.\n",
    "\n",
    "    Args:\n",
    "        y_true: Ground-truth binary labels (0 or 1).\n",
    "        y_scores: Continuous scores (e.g., probabilities or cosine similarity).\n",
    "        title: Chart title.\n",
    "        save_path: Path to save the PDF/PNG.\n",
    "        mark_threshold: Specific threshold value to mark on the curve with a dot.\n",
    "\n",
    "    Returns:\n",
    "        fpr, tpr, thresholds, auc_value\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    auc_value = auc(fpr, tpr)\n",
    "\n",
    "    # 1. Grid style (fixed as in the previous step — keep linewidth at 0.5)\n",
    "    custom_dotted_style = (0, (1, 1.5))\n",
    "    sns.set_theme(\n",
    "        style=\"darkgrid\",\n",
    "        rc={\n",
    "            \"grid.linestyle\": custom_dotted_style,\n",
    "            \"grid.linewidth\": 0.5,   # Your fixed setting\n",
    "            \"grid.color\": \"white\",\n",
    "            \"axes.edgecolor\": \"white\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4.0, 4.0), dpi=300)\n",
    "\n",
    "    # 2. Plot the data\n",
    "    ax.plot(fpr, tpr, color='C0', lw=2, label=f\"AUC = {auc_value:.3f}\")\n",
    "    ax.plot([0, 1], [0, 1], color='C1', linestyle='--', lw=1, label=\"Random\")\n",
    "\n",
    "    if mark_threshold is not None:\n",
    "        idx = np.argmin(np.abs(thresholds - mark_threshold))\n",
    "        ax.scatter(\n",
    "            fpr[idx],\n",
    "            tpr[idx],\n",
    "            s=25,\n",
    "            color='C0',\n",
    "            marker='o',\n",
    "            zorder=5,\n",
    "            label=f\"Threshold = {mark_threshold:.2f}\"\n",
    "        )\n",
    "\n",
    "    # --- 3. FONT COSMETICS (reduce sizes) ---\n",
    "\n",
    "    # Title: slightly smaller (usually 12–14, we use 11.5)\n",
    "    ax.set_title(title, fontsize=11.5)\n",
    "\n",
    "    # Axis labels: clearly smaller (we use 11.5)\n",
    "    ax.set_xlabel(\"False positive rate\", fontsize=11.5)\n",
    "    ax.set_ylabel(\"True positive rate\", fontsize=11.5)\n",
    "\n",
    "    # Tick labels (0.0, 0.2...): smaller (we use 11)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=11)\n",
    "\n",
    "    # Legend: smaller as well (we use 10.5)\n",
    "    ax.legend(loc=\"lower right\", fontsize=10.5)\n",
    "\n",
    "    # ---------------------------------------------\n",
    "\n",
    "    ax.set_xlim(-0.0, 1.0)\n",
    "    ax.set_ylim(-0.0, 1.02)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path is not None:\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        fig.savefig(save_path, bbox_inches=\"tight\")\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return fpr, tpr, thresholds, auc_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb560aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot\n",
    "if FULL_TEST:\n",
    "    fig_name = \"roc_labse_similarity_big_tuning\"\n",
    "else:\n",
    "    fig_name = \"roc_labse_similarity_small_tuning\"\n",
    "\n",
    "fig_path_labse = Path(f\"figures/{fig_name}.pdf\")\n",
    "best_threshold = 0.50 # chosen threshols\n",
    "\n",
    "fpr, tpr, ths, auc_val = plot_roc_curve_scores(\n",
    "    y_true=y_true,\n",
    "    y_scores=cosine_scores,\n",
    "    title=\"LaBSE - ROC curve\",\n",
    "    save_path=fig_path_labse,\n",
    "    mark_threshold=best_threshold\n",
    ")\n",
    "\n",
    "print(f\"LaBSE AUC: {auc_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b741791",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
